{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnTWheXsJHDyOlx+gYTTFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdallahs-Code/regex-automata-tool/blob/main/lexer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eFZxq1WQCfON"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from collections import deque\n",
        "import json\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import json\n",
        "import sys\n",
        "from graphviz import Digraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic NFA structures\n",
        "\n",
        "class State:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.transitions = {}  # key: symbol , value:list of State\n",
        "        self.is_terminal = False\n",
        "\n",
        "class NFA:\n",
        "    def __init__(self, start, end):\n",
        "        self.start = start      # start and end states\n",
        "        self.end = end\n",
        "\n",
        "class RegexToNFA:\n",
        "    def __init__(self, regex):\n",
        "        self.regex = regex\n",
        "        self.state_count = 0\n",
        "\n",
        "    def new_state(self):\n",
        "        name = f\"S{self.state_count}\"\n",
        "        self.state_count += 1\n",
        "        return State(name)\n",
        "\n",
        "    def symbol_nfa(self, symbol):\n",
        "        start = self.new_state()\n",
        "        end = self.new_state()\n",
        "        start.transitions[symbol] = [end]\n",
        "        return NFA(start, end)\n",
        "\n",
        "    def concat(self, nfa1, nfa2):\n",
        "        nfa1.end.transitions.setdefault('ε', []).append(nfa2.start)\n",
        "        return NFA(nfa1.start, nfa2.end)\n",
        "\n",
        "    def union(self, nfa1, nfa2):\n",
        "        start = self.new_state()\n",
        "        end = self.new_state()\n",
        "        start.transitions['ε'] = [nfa1.start, nfa2.start]\n",
        "        nfa1.end.transitions.setdefault('ε', []).append(end)\n",
        "        nfa2.end.transitions.setdefault('ε', []).append(end)\n",
        "        return NFA(start, end)\n",
        "\n",
        "    def star(self, nfa):\n",
        "        start = self.new_state()\n",
        "        end = self.new_state()\n",
        "        start.transitions['ε'] = [nfa.start, end]\n",
        "        nfa.end.transitions.setdefault('ε', []).extend([start, end])\n",
        "        return NFA(start, end)\n",
        "\n",
        "    def plus(self, nfa):\n",
        "        start = self.new_state()\n",
        "        end = self.new_state()\n",
        "        start.transitions['ε'] = [nfa.start]\n",
        "        nfa.end.transitions.setdefault('ε', []).extend([start, end])\n",
        "        return NFA(start, end)\n",
        "\n",
        "    def optional(self, nfa):\n",
        "        start = self.new_state()\n",
        "        end = self.new_state()\n",
        "        start.transitions['ε'] = [nfa.start, end]\n",
        "        nfa.end.transitions.setdefault('ε', []).append(end)\n",
        "        return NFA(start, end)\n",
        "\n",
        "    # old code for tokenizer\n",
        "    # def tokenize(self):\n",
        "    #     tokens = []\n",
        "    #     i = 0\n",
        "    #     while i < len(self.regex):\n",
        "    #         c = self.regex[i]\n",
        "    #         if c == '[':\n",
        "    #             j = i + 1\n",
        "    #             charset = set()\n",
        "    #             while j < len(self.regex) and self.regex[j] != ']':\n",
        "    #                 # Check that j+1 is '-', j+2 exists, AND j+2 is not ']'\n",
        "    #                 if (j + 2 < len(self.regex)) and self.regex[j + 1] == '-' and self.regex[j + 2] != ']':\n",
        "    #                     start_ch, end_ch = self.regex[j], self.regex[j + 2]\n",
        "    #                     charset.update(chr(k) for k in range(ord(start_ch), ord(end_ch) + 1))\n",
        "    #                     j += 3\n",
        "    #                 else:\n",
        "    #                     charset.add(self.regex[j])\n",
        "    #                     j += 1\n",
        "    #             if j >= len(self.regex) or self.regex[j] != ']':\n",
        "    #                 raise ValueError(\"Invalid regex: unmatched '['\")\n",
        "    #             tokens.append(charset)\n",
        "    #             i = j + 1\n",
        "    #         else:\n",
        "    #             tokens.append(c)\n",
        "    #             i += 1\n",
        "    #     return tokens\n",
        "\n",
        "\n",
        "    #Tokenizer\n",
        "    def tokenize(self):\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(self.regex):\n",
        "            c = self.regex[i]\n",
        "\n",
        "            if c == '[':\n",
        "                j = i + 1\n",
        "                square_bracket = c\n",
        "                while j < len(self.regex) and self.regex[j] != ']':\n",
        "                    square_bracket += self.regex[j]\n",
        "                    j += 1\n",
        "                square_bracket += ']'\n",
        "                if j >= len(self.regex) or self.regex[j] != ']':\n",
        "                    raise ValueError(\"Invalid regex: unmatched '['\")\n",
        "                tokens.append(square_bracket)\n",
        "                i = j + 1\n",
        "            else:\n",
        "                tokens.append(c)\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    #Insert concatenation operator (concat) so Shunting-yard algorithm works correctly\n",
        "    def insert_concat(self, tokens):\n",
        "        result = []\n",
        "        for i in range(len(tokens) - 1):\n",
        "            result.append(tokens[i])\n",
        "            t1, t2 = tokens[i], tokens[i + 1]\n",
        "            if ( t1.isalnum() or t1 =='.' or t1 in [')', '*', '+', '?'] or t1.endswith(']')) and\\\n",
        "               ( t2.isalnum() or t2 =='.' or t2 == '(' or t2.startswith('[') ):\n",
        "                result.append('concat')\n",
        "        result.append(tokens[-1])\n",
        "        # print(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def validate_token(self,token):\n",
        "        return  token =='.' or(token.isalnum() and token != 'concat')\\\n",
        "            or (token.startswith('[') and token.endswith(']'))\n",
        "\n",
        "    #Shunting-yard algorithm\n",
        "    def shunting_yard(self, tokens):\n",
        "        precedence = {'*': 3, '+': 3, '?': 3, 'concat': 2, '|': 1}\n",
        "        tokens = self.insert_concat(tokens)\n",
        "        postfix= []\n",
        "        stack = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if self.validate_token(token):\n",
        "                postfix.append(token)\n",
        "            elif token == '(':\n",
        "                stack.append(token)\n",
        "            elif token == ')':\n",
        "                while stack and stack[-1] != '(':\n",
        "                    postfix.append(stack.pop())\n",
        "                if not stack:\n",
        "                    raise ValueError(\"Mismatched parentheses\")\n",
        "                stack.pop()\n",
        "            else:     # token is concat or ? or * or + or |\n",
        "                while stack and stack[-1] != '(' and precedence[stack[-1]] >= precedence[token]:\n",
        "                    postfix.append(stack.pop())\n",
        "                stack.append(token)\n",
        "\n",
        "        while stack:\n",
        "            if stack[-1] in ['(', ')']:\n",
        "                raise ValueError(\"Mismatched parentheses\")\n",
        "            postfix.append(stack.pop())\n",
        "\n",
        "        return postfix\n",
        "\n",
        "\n",
        "    #Build NFA\n",
        "    def build(self):\n",
        "        tokens = self.tokenize()\n",
        "        tokens = self.shunting_yard(tokens)\n",
        "        # print(\"Postfix:\", tokens)\n",
        "        stack = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if self.validate_token(token):\n",
        "                stack.append(self.symbol_nfa(token))\n",
        "            elif token == 'concat':\n",
        "                nfa2 = stack.pop()\n",
        "                nfa1 = stack.pop()\n",
        "                stack.append(self.concat(nfa1, nfa2))\n",
        "            elif token == '|':\n",
        "                nfa2 = stack.pop()\n",
        "                nfa1 = stack.pop()\n",
        "                stack.append(self.union(nfa1, nfa2))\n",
        "            elif token == '*':\n",
        "                nfa = stack.pop()\n",
        "                stack.append(self.star(nfa))\n",
        "            elif token == '+':\n",
        "                nfa = stack.pop()\n",
        "                stack.append(self.plus(nfa))\n",
        "            elif token == '?':\n",
        "                nfa = stack.pop()\n",
        "                stack.append(self.optional(nfa))\n",
        "\n",
        "        nfa = stack.pop()\n",
        "        nfa.end.is_terminal = True\n",
        "        return nfa\n",
        "\n",
        "#This function performs a full traversal to find all states\n",
        "def get_all_states(start_state):\n",
        "        all_states = set()\n",
        "        queue = deque([start_state])\n",
        "        visited = set()\n",
        "\n",
        "        while queue:\n",
        "            s = queue.popleft()\n",
        "            if s in visited:\n",
        "                continue\n",
        "            visited.add(s)\n",
        "            all_states.add(s)\n",
        "\n",
        "            for _, targets in s.transitions.items():\n",
        "                for t in targets:\n",
        "                    if t not in visited:\n",
        "                        queue.append(t)\n",
        "        return all_states\n",
        "\n",
        "def renumber_states(nfa):\n",
        "\n",
        "    queue = deque([nfa.start])\n",
        "    visited = {nfa.start}\n",
        "    new_name_counter = 0\n",
        "\n",
        "    while queue:\n",
        "        current_state = queue.popleft()\n",
        "        current_state.name = f\"S{new_name_counter}\"\n",
        "        new_name_counter += 1\n",
        "\n",
        "        for _, targets in current_state.transitions.items():\n",
        "            for target in targets:\n",
        "                if target not in visited:\n",
        "                    visited.add(target)\n",
        "                    queue.append(target)\n",
        "\n",
        "#Convert NFA to json file\n",
        "def nfa_to_json(nfa):\n",
        "    states = {}\n",
        "    all_states = get_all_states(nfa.start)\n",
        "    states[\"startingState\"] = nfa.start.name\n",
        "\n",
        "    for s in all_states:\n",
        "        state_info = {\"isTerminatingState\": s.is_terminal}\n",
        "        for input, targets in s.transitions.items():\n",
        "            state_info[input] = [t.name for t in targets]\n",
        "        states[s.name] = state_info\n",
        "    return states"
      ],
      "metadata": {
        "id": "RNlx7tAquxDF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_closure(nfa, states):\n",
        "    stack = list(states) # Stack for traversing\n",
        "    closure = set(states) # Final set to be returned containing the states and all possible epsilon moves from them\n",
        "\n",
        "    while stack:\n",
        "        state = stack.pop()\n",
        "        if \"\\u03b5\" in nfa[state]:\n",
        "            eps = nfa[state][\"\\u03b5\"]\n",
        "            if isinstance(eps, str): # If there is only one epsilon going to one state make it a list\n",
        "                eps = [eps]\n",
        "\n",
        "            for next_state in eps:\n",
        "                if next_state not in closure:\n",
        "                    closure.add(next_state)\n",
        "                    stack.append(next_state) # Add to be explored so that if this successor state has another epsilon moves\n",
        "\n",
        "    return closure\n",
        "\n",
        "def get_input_symbols(nfa):\n",
        "    # Get all all possible inputs\n",
        "    symbols = set()\n",
        "\n",
        "    for state, info in nfa.items():\n",
        "        if state == \"startingState\":\n",
        "            continue\n",
        "\n",
        "        for key in info:\n",
        "            if key not in (\"isTerminatingState\", \"\\u03b5\"):\n",
        "                symbols.add(key)\n",
        "\n",
        "    return sorted(symbols)\n",
        "\n",
        "def convert_nfa_to_dfa(nfa):\n",
        "    start_state = nfa[\"startingState\"] # Add initial state of the nfa to the initial state of the new dfa\n",
        "    dfa_start = frozenset(epsilon_closure(nfa, {start_state})) # Add all states by epsilon moves\n",
        "\n",
        "    queue = [dfa_start] # Add initial state of dfa to the \"To be processed queue\"\n",
        "    dfa_states = {dfa_start} # Set to monitor all states\n",
        "\n",
        "    dfa = {} # Final dfa result\n",
        "    symbols = get_input_symbols(nfa)\n",
        "\n",
        "    while queue:\n",
        "        current = queue.pop(0)\n",
        "        dfa[current] = {}\n",
        "\n",
        "        is_accept = any(nfa[s][\"isTerminatingState\"] for s in current)\n",
        "        dfa[current][\"isTerminatingState\"] = is_accept # If any state is an accepting state, set the whole dfa state to final\n",
        "\n",
        "        for sym in symbols: # Loop on all possible inputs to this new dfa state\n",
        "            dest = set() # A set (state) that could be new or existing\n",
        "\n",
        "            for s in current: # Loop on all nfa states and see if they have this input\n",
        "                if sym in nfa[s]:\n",
        "                    next_state = nfa[s][sym]\n",
        "                    if isinstance(next_state, str):\n",
        "                        next_state = [next_state]\n",
        "\n",
        "                    for t in next_state:\n",
        "                        dest.add(t)\n",
        "\n",
        "            if dest: # We dont want to make an empty state in the dfa\n",
        "                new_state = frozenset(epsilon_closure(nfa, dest)) # Calculate new dfa state from the input sym to the previous dfa state\n",
        "                dfa[current][sym] = new_state # Set successor by input sym\n",
        "                if new_state not in dfa_states: # Dont make new state if it already exists\n",
        "                    dfa_states.add(new_state)\n",
        "                    queue.append(new_state)\n",
        "\n",
        "    return dfa, dfa_start\n",
        "\n",
        "def dfa_to_json(dfa, start_state):\n",
        "    # Convert output of the convert_nfa_to_dfa function to a good json format\n",
        "    result = {}\n",
        "    state_names = {}\n",
        "\n",
        "    for i, state in enumerate(dfa.keys()):\n",
        "        state_names[state] = f\"S{i}\" # Replacing states in dfa from for example s1,s2,s3 (nfa states) to s0 (dfa state)\n",
        "\n",
        "    result[\"startingState\"] = state_names[start_state]\n",
        "\n",
        "    for state, info in dfa.items():\n",
        "        name = state_names[state]\n",
        "        result[name] = {}\n",
        "        result[name][\"isTerminatingState\"] = info[\"isTerminatingState\"]\n",
        "\n",
        "        for key, target in info.items():\n",
        "            if key == \"isTerminatingState\":\n",
        "                continue\n",
        "            result[name][key] = state_names[target]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "wX-BOjiEaGIj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the dfa is generated but not minimized"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hfv2GZTLHSHF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minimize_dfa(dfa_json):\n",
        "    states = [s for s in dfa_json if s != \"startingState\"] # Extracting dfa states\n",
        "    start_state = dfa_json[\"startingState\"]\n",
        "    symbols = get_input_symbols(dfa_json) # Collecting symbols\n",
        "\n",
        "    # Dividing initially onto two groups of accepting and non-accepting states\n",
        "    accepting = set(s for s in states if dfa_json[s][\"isTerminatingState\"])\n",
        "    non_accepting = set(s for s in states if not dfa_json[s][\"isTerminatingState\"])\n",
        "\n",
        "    # Handle the case of all states being accepting\n",
        "    groups = []\n",
        "    if accepting:\n",
        "        groups.append(accepting)\n",
        "    if non_accepting:\n",
        "        groups.append(non_accepting)\n",
        "\n",
        "    changed = True # If groups have been divided then iterate again\n",
        "\n",
        "    while changed:\n",
        "        changed = False\n",
        "        new_groups = [] # Future groups for the next iteration\n",
        "\n",
        "        for group in groups:\n",
        "            # If group has less than or equal 1 state, then we cannot split\n",
        "            if len(group) <= 1:\n",
        "                new_groups.append(group)\n",
        "                continue # Skip this group\n",
        "            groups_by_signature = {} # signature -> list of states having the same signature\n",
        "\n",
        "            for state in group:\n",
        "                signature = [] # signature: list where i (symbol) maps to element (successor group for this input symbol)\n",
        "\n",
        "                for sym in symbols:\n",
        "                    if sym in dfa_json[state]:\n",
        "                        target = dfa_json[state][sym] # Find which group contains the target state\n",
        "\n",
        "                        for i, g in enumerate(groups):\n",
        "                            if target in g:\n",
        "                                signature.append(i)\n",
        "                                break\n",
        "                    else:\n",
        "                        signature.append(-1) # This state doesnt has the input sym\n",
        "\n",
        "                signature = tuple(signature)\n",
        "                if signature not in groups_by_signature:\n",
        "                    groups_by_signature[signature] = []\n",
        "                groups_by_signature[signature].append(state) # Append state to other states with the same signature\n",
        "            if len(groups_by_signature) > 1: # Indicating this group have been splitted\n",
        "                changed = True # We will try again next iteration\n",
        "\n",
        "            for g in groups_by_signature.values():\n",
        "                new_groups.append(set(g)) # Prepare the new splitted groups for the next iteration\n",
        "\n",
        "        groups = new_groups\n",
        "\n",
        "    group_names = {} # Naming each group with a single state like s1 s2 s3 -> s0\n",
        "    result = {} # Building the final result\n",
        "\n",
        "    # Naming states in the group containing the initial state s0\n",
        "    start_group_idx = None\n",
        "\n",
        "    for idx, group in enumerate(groups):\n",
        "        if start_state in group:\n",
        "\n",
        "            for state in group:\n",
        "                group_names[state] = f\"S0\"\n",
        "            start_group_idx = idx\n",
        "            break\n",
        "\n",
        "    # Naming other states\n",
        "    current_idx = 1\n",
        "\n",
        "    for idx, group in enumerate(groups):\n",
        "        if idx == start_group_idx:\n",
        "            continue\n",
        "\n",
        "        for state in group:\n",
        "            group_names[state] = f\"S{current_idx}\"\n",
        "        current_idx += 1\n",
        "\n",
        "    result[\"startingState\"] = group_names[start_state]\n",
        "\n",
        "    for idx, group in enumerate(groups):\n",
        "        rep = next(iter(group)) # Pick a representative to the group\n",
        "        result[group_names[rep]] = {}\n",
        "        is_accepting = any(dfa_json[s][\"isTerminatingState\"] for s in group) # Checking if this state is a final state\n",
        "        result[group_names[rep]][\"isTerminatingState\"] = is_accepting\n",
        "\n",
        "        for sym in symbols:\n",
        "            if sym in dfa_json[rep]:\n",
        "                old_target = dfa_json[rep][sym]\n",
        "                result[group_names[rep]][sym] = group_names[old_target] # Setting the successor state for this input\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "T0HBRUTveY--"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_graph(json_file, output_file=\"nfa_graph\"):\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        nfa_data = json.load(f)\n",
        "\n",
        "    starting_state = nfa_data[\"startingState\"]\n",
        "\n",
        "    dot = Digraph(comment=\"NFA Graph\", format=\"png\")\n",
        "    dot.attr(rankdir=\"LR\")\n",
        "\n",
        "    for state_name, info in nfa_data.items():\n",
        "        if state_name == \"startingState\":\n",
        "            continue\n",
        "\n",
        "        is_terminating = info.get(\"isTerminatingState\", False)\n",
        "        shape = \"doublecircle\" if is_terminating else \"circle\"\n",
        "\n",
        "\n",
        "        fillcolor = \"white\"\n",
        "\n",
        "        dot.node(state_name, state_name, shape=shape, style=\"filled\", fillcolor=fillcolor)\n",
        "\n",
        "\n",
        "    dot.node(\"\", shape=\"none\", height=\"0\", width=\"0\") # Invisible entry node\n",
        "    dot.edge(\"\", starting_state, label=\"Start\", color=\"gray\")\n",
        "\n",
        "\n",
        "    for state_name, info in nfa_data.items():\n",
        "        if state_name == \"startingState\":\n",
        "            continue\n",
        "\n",
        "        for symbol, target_list in info.items():\n",
        "            # Skip non-transition keys\n",
        "            if symbol in [\"isTerminatingState\"]:\n",
        "                continue\n",
        "\n",
        "            if isinstance(target_list, list):\n",
        "                for target_state in target_list:\n",
        "                    dot.edge(state_name, target_state, label=str(symbol))\n",
        "            else:\n",
        "                dot.edge(state_name, str(target_list), label=str(symbol))\n",
        "\n",
        "    dot.render(output_file, cleanup=True)"
      ],
      "metadata": {
        "id": "bSTX8wd9x1mR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_to_dfa(regex):\n",
        "  converter = RegexToNFA(regex)\n",
        "  nfa = converter.build()\n",
        "  renumber_states(nfa)\n",
        "\n",
        "  json_output = nfa_to_json(nfa) # Json nfa\n",
        "\n",
        "  with open(\"nfa.json\", \"w\") as f: # Json nfa file\n",
        "      json.dump(json_output, f, indent=2)\n",
        "\n",
        "  output_name = \"nfa_graph\"\n",
        "  draw_graph(\"nfa.json\", output_name) # Nfa graph\n",
        "\n",
        "  nfa = json_output\n",
        "\n",
        "  dfa, dfa_start = convert_nfa_to_dfa(nfa)\n",
        "  dfa_json = dfa_to_json(dfa, dfa_start)\n",
        "\n",
        "  result = minimize_dfa(dfa_json) # Json dfa\n",
        "\n",
        "  with open(\"dfa.json\", \"w\") as f: # Json dfa file\n",
        "      json.dump(result, f, indent=2)\n",
        "\n",
        "  output_name = \"dfa_graph\"\n",
        "  draw_graph(\"dfa.json\", output_name) # Dfa graph"
      ],
      "metadata": {
        "id": "g7O1dafCyZ0i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex = input(\"Enter regex: \")\n",
        "regex_to_dfa(regex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh-odPjA1eKS",
        "outputId": "08810591-ed9f-4ba9-d7e3-a8e3ccd4a7d8",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter regex: a*\n"
          ]
        }
      ]
    }
  ]
}